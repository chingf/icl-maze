defaults:
  - model: transformer_end_query
  - env: tree
  - optimizer: default  # For finding the relevant trained model
  - _self_

wandb:
  project: 7layer #lazyload

model:
  dropout: 0

env:
  n_envs: 300000

#optimizer:
#  lr: 1e-5
#  batch_size: 256

seed: 0
epoch: -1 #best
n_eval_envs: 25 #50 # (~per-mouse, each environment contains a different interaction history)
test_horizon: 400  # (horizon for each evaluation episode)

# Eval-specific parameters
offline_eval_episodes: 5 #15 # (per environment)  # TODO
online_eps_in_context: 1 # (number of episodes to keep in context)
online_eval_episodes: 40 # (number of sequential episodes to run)

storage_dir: /n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze
#storage_dir: /n/holylabs/LABS/krajan_lab/Lab/cfang/icl-maze

override_eval_dataset_path: /n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/lazyload/tree_layers7_bprob1.0_envs600000_H1600_explore/datasets/eval.h5
override_params:
  - branching_prob: 1.0
