{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "import random\n",
    "from src.evals.eval_trees import EvalTrees\n",
    "from src.evals.eval_trees import EvalCntrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_type = 'far'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13-val_loss=0.911523.ckpt\n"
     ]
    }
   ],
   "source": [
    "corr = 0.25\n",
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"darkroom_simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "# Extract correlation and state_dim from eval dataset path\n",
    "state_dim = int(re.search(r'state_dim(\\d+)', eval_dset_path).group(1))\n",
    "maze_dim = int(re.search(r'_dim(\\d+)_corr', eval_dset_path).group(1))\n",
    "node_encoding_corr = float(re.search(r'corr(\\d*\\.?\\d*)', eval_dset_path).group(1))\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": 5,\n",
    "    \"dropout\": dropout,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3173948/4170590101.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_pkl)\n"
     ]
    }
   ],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model_config['initialization_seed'] = 0\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_envs = 50\n",
    "\n",
    "is_h5_file = eval_dset_path.endswith('.h5')\n",
    "if is_h5_file:\n",
    "    eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    traj_indices = list(eval_trajs.keys())\n",
    "    n_eval_envs = min(n_eval_envs, len(traj_indices))\n",
    "    random.seed(0)\n",
    "    traj_indices = random.sample(traj_indices, n_eval_envs)\n",
    "    random.seed()\n",
    "    eval_trajs = [eval_trajs[i] for i in traj_indices]\n",
    "else:  # Pickle file\n",
    "    with open(eval_dset_path, 'rb') as f:\n",
    "        eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = min(n_eval_envs, len(eval_trajs))\n",
    "    random.seed(0)\n",
    "    eval_trajs = random.sample(eval_trajs, n_eval_envs)\n",
    "    random.seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [  # (env, seed, figname)\n",
    "    (8, 0, 'L_20A_1'),\n",
    "    (9, 1, 'L_20A_2'),\n",
    "    (12, 0, 'L_20A_3'),\n",
    "    (0, 1, 'L_20A_4'),\n",
    "\n",
    "]\n",
    "example_idx = 3\n",
    "i_eval = examples[example_idx][0]\n",
    "np.random.seed(examples[example_idx][1])\n",
    "figname = examples[example_idx][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.envs.darkroom import DarkroomEnv\n",
    "\n",
    "traj = eval_trajs[i_eval]\n",
    "env_config = {\n",
    "    'maze_dim': maze_dim,\n",
    "    'horizon': 200,\n",
    "    'state_dim': state_dim,\n",
    "    'node_encoding_corr': node_encoding_corr,\n",
    "    'initialization_seed': np.array(traj['initialization_seed']).item(),\n",
    "    'goal': np.array(traj['goal'])\n",
    "}\n",
    "env = DarkroomEnv(**env_config)\n",
    "optimal_action_map, dist_from_goal = env.make_opt_action_dict()\n",
    "valid_query_states = []\n",
    "for i in range(len(traj['context_states'])):\n",
    "    d = dist_from_goal[tuple(traj['context_states'][i].tolist())]\n",
    "    if query_type == 'far' and d < 6:\n",
    "        continue\n",
    "    elif query_type == 'middle' and d != 5:\n",
    "       continue\n",
    "    valid_query_states.append(traj['context_states'][i])\n",
    "if len(valid_query_states) == 0:\n",
    "    raise ValueError('No valid query states found')\n",
    "else:\n",
    "    traj['query_state'] = valid_query_states[np.random.choice(len(valid_query_states))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 50, Reward: 0\n",
      "Context length: 100, Reward: 1\n",
      "Context length: 150, Reward: 7\n",
      "First nonzero reward at step 52: 1\n"
     ]
    }
   ],
   "source": [
    "for seq_length in [50, 100, 150]:\n",
    "    print(f'Context length: {seq_length}, Reward: {traj['context_rewards'][:seq_length].sum()}')\n",
    "\n",
    "first_reward_idx = np.where(traj['context_rewards'] != 0)[0][0]\n",
    "print(f'First nonzero reward at step {first_reward_idx}: {traj['context_rewards'][first_reward_idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Gradient Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transformer_input_from_batch(batch, model):\n",
    "    query_states = batch['query_states'][:, None, :]\n",
    "    zeros = batch['zeros'][:, None, :]\n",
    "    state_seq = torch.cat([batch['context_states'], query_states], dim=1)\n",
    "    action_seq = torch.cat(\n",
    "        [batch['context_actions'], zeros[:, :, :model.action_dim]], dim=1)\n",
    "    next_state_seq = torch.cat(\n",
    "        [batch['context_next_states'], zeros[:, :, :model.state_dim]], dim=1)\n",
    "    reward_seq = torch.cat([batch['context_rewards'], zeros[:, :, :1]], dim=1)\n",
    "    seq = torch.cat(\n",
    "        [state_seq, action_seq, next_state_seq, reward_seq], dim=2)\n",
    "    seq_len = seq.shape[1]\n",
    "    stacked_inputs = model.embed_transition(seq)\n",
    "    return stacked_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batches(traj):\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([np.array(traj['context_states'][:context_length])]),\n",
    "        'context_actions': convert_to_tensor([np.array(traj['context_actions'][:context_length])]),\n",
    "        'context_next_states': convert_to_tensor([np.array(traj['context_next_states'][:context_length])]),\n",
    "        'context_rewards': convert_to_tensor([np.array(traj['context_rewards'][:context_length])[:, None]]),\n",
    "        'query_states': convert_to_tensor([np.array(traj['query_state'])]),\n",
    "        } \n",
    "    batch['zeros'] = torch.zeros(1, 10 ** 2 + 4 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        if 'context' in k:\n",
    "            batch[k] = batch[k]\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "    baseline_batch = {}\n",
    "    for k, v in batch.items():\n",
    "        baseline_batch[k] = v.clone() if isinstance(v, torch.Tensor) else v\n",
    "    baseline_batch['context_actions'] *= 0\n",
    "    #baseline_batch['context_actions'] += 0.25\n",
    "    return baseline_batch, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_batch, batch = format_batches(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = {}\n",
    "hooks = []\n",
    "\n",
    "def get_hook(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[layer_name] = output\n",
    "    return hook\n",
    "\n",
    "for attn_layer, module in enumerate(model.transformer.h):\n",
    "    module.register_forward_hook(get_hook(attn_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj['optimal_action'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    final_output = model(batch)\n",
    "    prediction = final_output[:, traj['optimal_action'].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attributions for each layer\n",
    "layer_attributions = {}\n",
    "inputs = format_transformer_input_from_batch(batch, model)\n",
    "baseline_inputs = format_transformer_input_from_batch(baseline_batch, model)\n",
    "\n",
    "for layer_name, layer_output in layer_outputs.items():\n",
    "    # Prepare interpolated inputs\n",
    "    alphas = torch.linspace(0, 1, steps=20)\n",
    "    \n",
    "    all_grads = []\n",
    "    for alpha in alphas:\n",
    "        interp_input = alpha*inputs + (1-alpha)*baseline_inputs\n",
    "        interp_input.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass to get layer output\n",
    "        transformer_output = model.transformer(inputs_embeds=interp_input)\n",
    "        preds = model.pred_actions(transformer_output['last_hidden_state'])\n",
    "        preds = preds[:, -1, :]\n",
    "        target = preds[:, traj['optimal_action'].argmax()]\n",
    "        current_layer_output = layer_outputs[layer_name][0]\n",
    "\n",
    "        grad_wrt_layer = torch.autograd.grad(\n",
    "           outputs=target,\n",
    "           inputs=current_layer_output,\n",
    "           grad_outputs=torch.ones_like(target),\n",
    "           retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        grad_wrt_input = torch.autograd.grad(\n",
    "            outputs=current_layer_output[:, -1],\n",
    "            inputs=interp_input,\n",
    "            grad_outputs=current_layer_output[:, -1],\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        all_grads.append(grad_wrt_input)\n",
    "    \n",
    "    # Average gradients and compute attribution\n",
    "    avg_grad = torch.stack(all_grads).mean(dim=0)\n",
    "    avg_grad = avg_grad.detach().cpu().numpy().squeeze()\n",
    "    delta_input = (inputs - baseline_inputs).detach().cpu().numpy().squeeze()\n",
    "    attribution = np.sum(avg_grad * delta_input, axis=1)\n",
    "    layer_attributions[layer_name] = attribution\n",
    "\n",
    "# Output w.r.t Input\n",
    "alphas = torch.linspace(0, 1, steps=20)\n",
    "all_grads = []\n",
    "for alpha in alphas:\n",
    "    interp_input = alpha*inputs + (1-alpha)*baseline_inputs\n",
    "    interp_input.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass to get layer output\n",
    "    transformer_output = model.transformer(inputs_embeds=interp_input)\n",
    "    preds = model.pred_actions(transformer_output['last_hidden_state'])\n",
    "    preds = preds[:, -1, :]\n",
    "    target = preds[:, traj['optimal_action'].argmax()]\n",
    "    current_layer_output = layer_outputs[layer_name][0]\n",
    "\n",
    "    grad_wrt_input = torch.autograd.grad(\n",
    "        outputs=target,\n",
    "        inputs=interp_input,\n",
    "        grad_outputs=target,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    all_grads.append(grad_wrt_input)\n",
    "avg_grad = torch.stack(all_grads).mean(dim=0)\n",
    "avg_grad = avg_grad.detach().cpu().numpy().squeeze()\n",
    "delta_input = (inputs - baseline_inputs).detach().cpu().numpy().squeeze()\n",
    "input_to_output_attribution = np.sum(avg_grad * delta_input, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import networkx as nx\n",
    "\n",
    "def plot_trajectory(\n",
    "        states, next_states, query_state, attentions, env, figname=None):\n",
    "    \n",
    "    G = env.to_networkx()\n",
    "    pos = {node: node for node in G.nodes()}\n",
    "    node_size = 15\n",
    "    linewidth = 1\n",
    "    goal_state = env.node_map_encoding_to_pos[tuple(env.goal.tolist())]\n",
    "    query_state = env.node_map_encoding_to_pos[tuple(query_state.tolist())]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1.35, 1.35))  # Create a new figure\n",
    "    nx.draw(G, pos,\n",
    "            nodelist=[n for n in G.nodes() if n != goal_state],\n",
    "            node_color='gray',\n",
    "            edge_color='white',\n",
    "            node_size=node_size,\n",
    "            font_size=8,\n",
    "            font_weight='bold',\n",
    "            width=1,\n",
    "            alpha=1.)\n",
    "\n",
    "    # Then draw specific nodes in green\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[query_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size)\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[goal_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size,\n",
    "                      node_shape='*')\n",
    "    \n",
    "    for state, next_state, attention in zip(states, next_states, attentions):\n",
    "        edge_color = 'red' if attention > 0 else 'blue'\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "            edgelist=[(tuple(state), tuple(next_state))],\n",
    "            edge_color=edge_color,\n",
    "            alpha=abs(attention),\n",
    "            width=linewidth)\n",
    "        \n",
    "    # Replace plt.axis('off') with code to add a box outline\n",
    "    plt.axis('on')  # Turn on axes\n",
    "    ax.set_frame_on(True)  # Make sure frame is on\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1) \n",
    "    \n",
    "    # Remove tick marks but keep the box\n",
    "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    \n",
    "    # Adjust limits to fit the graph with some padding\n",
    "    x_min, x_max = min(n[0] for n in G.nodes()), max(n[0] for n in G.nodes())\n",
    "    y_min, y_max = min(n[1] for n in G.nodes()), max(n[1] for n in G.nodes())\n",
    "    padding = 0.5\n",
    "    ax.set_xlim(x_min - padding, x_max + padding)\n",
    "    ax.set_ylim(y_min - padding, y_max + padding)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if figname is not None:\n",
    "        plt.savefig(f'figs_app/{figname}.png', transparent=True, dpi=300)\n",
    "        plt.savefig(f'figs_app/{figname}.pdf', transparent=True, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "next_states = []\n",
    "\n",
    "state_features = traj['context_states'][:context_length].squeeze()\n",
    "next_state_features = traj['context_next_states'][:context_length].squeeze()\n",
    "for state_feature, next_state_feature in zip(state_features, next_state_features):\n",
    "    state_feature = tuple(state_feature.tolist())\n",
    "    next_state_feature = tuple(next_state_feature.tolist())\n",
    "    states.append(list(env.node_map_encoding_to_pos[state_feature]))\n",
    "    next_states.append(list(env.node_map_encoding_to_pos[next_state_feature]))\n",
    "    \n",
    "states = np.array(states)\n",
    "next_states = np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_and_normalize(attentions, pctile=100):\n",
    "    max_scale = np.percentile(np.abs(attentions), pctile)\n",
    "    attentions = np.clip(attentions, 0, max_scale)/max_scale\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 135x135 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attentions = input_to_output_attribution[:-1]\n",
    "idx_to_plot = [False if np.array_equal(states[i], next_states[i]) else True for i in range(len(states))]\n",
    "attentions = np.abs(attentions)\n",
    "attentions = clip_and_normalize(attentions)[idx_to_plot]\n",
    "plot_trajectory(\n",
    "    states[idx_to_plot], next_states[idx_to_plot], traj['query_state'],\n",
    "    attentions,\n",
    "    env,\n",
    "    figname=figname\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
