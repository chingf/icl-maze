{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "import random\n",
    "from src.envs.darkroom import DarkroomEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13-val_loss=0.911523.ckpt\n"
     ]
    }
   ],
   "source": [
    "corr = 0.25\n",
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"darkroom_simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "# Extract correlation and state_dim from eval dataset path\n",
    "state_dim = int(re.search(r'state_dim(\\d+)', eval_dset_path).group(1))\n",
    "maze_dim = int(re.search(r'_dim(\\d+)_corr', eval_dset_path).group(1))\n",
    "node_encoding_corr = float(re.search(r'corr(\\d*\\.?\\d*)', eval_dset_path).group(1))\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": 5,\n",
    "    \"dropout\": dropout,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3383041/4170590101.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_pkl)\n"
     ]
    }
   ],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model_config['initialization_seed'] = 0\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_envs = 50\n",
    "\n",
    "is_h5_file = eval_dset_path.endswith('.h5')\n",
    "if is_h5_file:\n",
    "    eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    traj_indices = list(eval_trajs.keys())\n",
    "    n_eval_envs = min(n_eval_envs, len(traj_indices))\n",
    "    random.seed(0)\n",
    "    traj_indices = random.sample(traj_indices, n_eval_envs)\n",
    "    random.seed()\n",
    "    eval_trajs = [eval_trajs[i] for i in traj_indices]\n",
    "else:  # Pickle file\n",
    "    with open(eval_dset_path, 'rb') as f:\n",
    "        eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = min(n_eval_envs, len(eval_trajs))\n",
    "    random.seed(0)\n",
    "    eval_trajs = random.sample(eval_trajs, n_eval_envs)\n",
    "    random.seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env_config = {\n",
    "    'maze_dim': maze_dim,\n",
    "    'horizon': 200,\n",
    "    'state_dim': state_dim,\n",
    "    'node_encoding_corr': node_encoding_corr,\n",
    "    'initialization_seed': None, # will be overwritten\n",
    "    'goal': None # will be overwritten\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test goals and trajectories in XY space\n",
    "Goal location and long-path trajectories will be overwritten in each test environment, so that we can aggregate performance in an automated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [3 0]]\n"
     ]
    }
   ],
   "source": [
    "test_xy_goals = []\n",
    "for i_eval in range(n_eval_envs):\n",
    "    traj = eval_trajs[i_eval]\n",
    "    base_env_config['initialization_seed'] = np.array(traj['initialization_seed']).item()\n",
    "    base_env_config['goal'] = np.array(traj['goal'])\n",
    "    env = DarkroomEnv(**base_env_config)\n",
    "    xy_goal = env.node_map_encoding_to_pos[tuple(env.goal.tolist())]\n",
    "    test_xy_goals.append(xy_goal)\n",
    "\n",
    "print(np.unique(test_xy_goals, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xy_goals = np.unique(test_xy_goals, axis=0)\n",
    "test_xy_trajectories = [\n",
    "    [[0,0], [0,1], [1,1], [2,1], [3,1], [4,1], [4,2], [3,2], [2,2]],\n",
    "    [[0,3], [1,3], [2,3], [3,3], [4,3], [4,2], [4,1], [4,0], [3,0]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 1\n",
    "test_xy_goal = test_xy_goals[test_idx]\n",
    "test_xy_trajectory = test_xy_trajectories[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_eval = 2\n",
    "traj = eval_trajs[i_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'maze_dim': maze_dim,\n",
    "    'horizon': 200,\n",
    "    'state_dim': state_dim,\n",
    "    'node_encoding_corr': node_encoding_corr,\n",
    "    'initialization_seed': np.array(traj['initialization_seed']).item(),\n",
    "    'goal': np.array(traj['goal'])\n",
    "}\n",
    "env = DarkroomEnv(**env_config)\n",
    "env.goal = np.array(env.node_map_pos_to_encoding[tuple(test_xy_goal)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 1\n",
    "full_test_xy_trajectory = []\n",
    "\n",
    "for _ in range(repeats):\n",
    "    full_test_xy_trajectory.extend(deepcopy(test_xy_trajectory))\n",
    "    full_test_xy_trajectory.extend(deepcopy(test_xy_trajectory[:-1][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onestep_action(xy1, xy2):\n",
    "    x1, y1 = xy1\n",
    "    x2, y2 = xy2\n",
    "    assert np.abs(x1-x2) + np.abs(y1-y2) <= 1\n",
    "    if xy1 == xy2:\n",
    "        return 4\n",
    "    elif x1 < x2 and y1 == y2:\n",
    "        return 2\n",
    "    elif x1 > x2 and y1 == y2:\n",
    "        return 0\n",
    "    elif x1 == x2 and y1 < y2:\n",
    "        return 1\n",
    "    elif x1 == x2 and y1 > y2:\n",
    "        return 3\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action: {xy1} -> {xy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory_batch(xy_trajectory, xy_goal, env):\n",
    "    context_states = []\n",
    "    context_actions = []\n",
    "    context_next_states = []\n",
    "    context_rewards = []\n",
    "    query_state = []\n",
    "    for i in range(len(xy_trajectory)-1):\n",
    "        state_feature = np.array(env.node_map_pos_to_encoding[tuple(xy_trajectory[i])])\n",
    "        next_state_feature = np.array(env.node_map_pos_to_encoding[tuple(xy_trajectory[i+1])])\n",
    "        action_idx = get_onestep_action(xy_trajectory[i], xy_trajectory[i+1])\n",
    "        action = np.zeros(5)\n",
    "        action[action_idx] = 1\n",
    "        reward = 1 if np.all(xy_trajectory[i+1] == xy_goal) else 0\n",
    "        context_states.append(state_feature)\n",
    "        context_actions.append(action)\n",
    "        context_next_states.append(next_state_feature)\n",
    "        context_rewards.append(reward)\n",
    "    context_states = np.array(context_states).squeeze()\n",
    "    context_actions = np.array(context_actions)\n",
    "    context_next_states = np.array(context_next_states).squeeze()\n",
    "    context_rewards = np.array(context_rewards)\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([context_states]),\n",
    "        'context_actions': convert_to_tensor([context_actions]),\n",
    "        'context_next_states': convert_to_tensor([context_next_states]),\n",
    "        'context_rewards': convert_to_tensor([context_rewards[:, None]]),\n",
    "    }\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_trajectory_batch(full_test_xy_trajectory, test_xy_goal, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3) to (1, 3) with action 2 and reward 0.0\n",
      "(1, 3) to (2, 3) with action 2 and reward 0.0\n",
      "(2, 3) to (3, 3) with action 2 and reward 0.0\n",
      "(3, 3) to (4, 3) with action 2 and reward 0.0\n",
      "(4, 3) to (4, 2) with action 3 and reward 0.0\n",
      "(4, 2) to (4, 1) with action 3 and reward 0.0\n",
      "(4, 1) to (4, 0) with action 3 and reward 0.0\n",
      "(4, 0) to (3, 0) with action 0 and reward 1.0\n",
      "(3, 0) to (4, 0) with action 2 and reward 0.0\n",
      "(4, 0) to (4, 1) with action 1 and reward 0.0\n",
      "(4, 1) to (4, 2) with action 1 and reward 0.0\n",
      "(4, 2) to (4, 3) with action 1 and reward 0.0\n",
      "(4, 3) to (3, 3) with action 0 and reward 0.0\n",
      "(3, 3) to (2, 3) with action 0 and reward 0.0\n",
      "(2, 3) to (1, 3) with action 0 and reward 0.0\n",
      "(1, 3) to (0, 3) with action 0 and reward 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch['context_states'].shape[1]):\n",
    "    start = batch['context_states'][:, i].cpu().numpy().squeeze()\n",
    "    next = batch['context_next_states'][:, i].cpu().numpy().squeeze()\n",
    "    action = np.argmax(batch['context_actions'][:, i].cpu().numpy())\n",
    "    reward = batch['context_rewards'][:, i].cpu().numpy().squeeze()\n",
    "    start = env.node_map_encoding_to_pos[tuple(start.tolist())]\n",
    "    next = env.node_map_encoding_to_pos[tuple(next.tolist())]\n",
    "    print(f'{start} to {next} with action {action} and reward {reward}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "curr_xy_state = full_test_xy_trajectory[-1]\n",
    "query_state = env.node_map_pos_to_encoding[tuple(curr_xy_state)]\n",
    "xy_path = [curr_xy_state]\n",
    "chosen_actions = []\n",
    "experienced_rewards = []\n",
    "\n",
    "# Convert initial context tensors to numpy for easier concatenation\n",
    "context_states = batch['context_states'].cpu().numpy().squeeze()\n",
    "context_actions = batch['context_actions'].cpu().numpy().squeeze()\n",
    "context_next_states = batch['context_next_states'].cpu().numpy().squeeze()\n",
    "context_rewards = batch['context_rewards'].cpu().numpy().squeeze()\n",
    "\n",
    "for i in range(n_steps):\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([context_states]),\n",
    "        'context_actions': convert_to_tensor([context_actions]),\n",
    "        'context_next_states': convert_to_tensor([context_next_states]),\n",
    "        'context_rewards': convert_to_tensor([context_rewards[:, None]]),\n",
    "        }\n",
    "    batch['query_states'] = convert_to_tensor([np.array(query_state)])\n",
    "    batch['query_states'] = batch['query_states'].to(model.device)\n",
    "    batch['zeros'] = torch.zeros(1, state_dim ** 2 + 5 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "\n",
    "    pred_action = torch.argmax(out.squeeze()).item()\n",
    "    action_encoding = np.zeros(5)\n",
    "    action_encoding[pred_action] = 1\n",
    "    next_state_encoding, reward = env.transit(np.array(query_state), action_encoding)\n",
    "\n",
    "    context_states = np.vstack([context_states, query_state])\n",
    "    context_actions = np.vstack([context_actions, action_encoding])\n",
    "    context_next_states = np.vstack([context_next_states, next_state_encoding])\n",
    "    context_rewards = np.append(context_rewards, reward)\n",
    "\n",
    "    xy_path.append(env.node_map_encoding_to_pos[tuple(next_state_encoding)])\n",
    "    chosen_actions.append(pred_action)\n",
    "    experienced_rewards.append(reward)\n",
    "    query_state = next_state_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3],\n",
       " (0, 2),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (3, 0),\n",
       " (3, 0),\n",
       " (3, 0),\n",
       " (3, 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import viridis\n",
    "\n",
    "node_size = 15\n",
    "\n",
    "def plot_trajectory(\n",
    "        G,\n",
    "        observed_path,\n",
    "        taken_path,\n",
    "        savefig=False\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot a trajectory through a networkx graph with color-coded transitions.\n",
    "    \n",
    "    Parameters:\n",
    "    - G: networkx graph\n",
    "    - trajectory: list of node IDs representing the path\n",
    "    - pos: optional dictionary of node positions\n",
    "    \"\"\"\n",
    "\n",
    "    observed_path = [tuple(node) for node in observed_path]\n",
    "    taken_path = [tuple(node) for node in taken_path]\n",
    "    pos = {node: node for node in G.nodes()}\n",
    "    goal_state = env.node_map_encoding_to_pos[tuple(env.goal.tolist())] \n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(1.42, 1.42))  # Create a new figure\n",
    "    nx.draw(G, pos,\n",
    "            nodelist=[n for n in G.nodes() if n != goal_state],\n",
    "           node_color='gray',\n",
    "           edge_color='white',\n",
    "           node_size=node_size,\n",
    "           font_size=8,\n",
    "           font_weight='bold',\n",
    "           width=1,\n",
    "           alpha=1.,\n",
    "           ax=ax)\n",
    "    \n",
    "    # Draw each transition with a color based on its timestep\n",
    "    for i in range(len(observed_path)-1):\n",
    "        start_node = observed_path[i]\n",
    "        end_node = observed_path[i+1]\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "            edgelist=[(start_node, end_node)],\n",
    "            edge_color='orange',\n",
    "            style='dashed',\n",
    "            alpha=0.8,\n",
    "            width=2,\n",
    "            ax=ax)\n",
    "        \n",
    "    for i in range(len(taken_path)-1):\n",
    "        start_node = taken_path[i]\n",
    "        end_node = taken_path[i+1]\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "            edgelist=[(start_node, end_node)],\n",
    "            edge_color='blue',\n",
    "            alpha=0.8,\n",
    "            width=2,\n",
    "            ax=ax)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[goal_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size,\n",
    "                      node_shape='*',\n",
    "                      ax=ax)\n",
    "    start_state = observed_path[0]\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[start_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size,\n",
    "                      ax=ax)\n",
    "    \n",
    "    # Replace plt.axis('off') with code to add a box outline\n",
    "    plt.axis('on')  # Turn on axes\n",
    "    ax.set_frame_on(True)  # Make sure frame is on\n",
    "    \n",
    "    # Remove tick marks but keep the box\n",
    "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    \n",
    "    # Adjust limits to fit the graph with some padding\n",
    "    x_min, x_max = min(n[0] for n in G.nodes()), max(n[0] for n in G.nodes())\n",
    "    y_min, y_max = min(n[1] for n in G.nodes()), max(n[1] for n in G.nodes())\n",
    "    padding = 0.5\n",
    "    ax.set_xlim(x_min - padding, x_max + padding)\n",
    "    ax.set_ylim(y_min - padding, y_max + padding)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(f'figs/1c.png', transparent=True, dpi=300)\n",
    "        plt.savefig(f'figs/1c.svg', transparent=True, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 142x142 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_trajectory(\n",
    "    env.to_networkx(),\n",
    "    observed_path=test_xy_trajectory,\n",
    "    taken_path=xy_path,\n",
    "    savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 0\n",
    "test_xy_goal = test_xy_goals[test_idx]\n",
    "test_xy_trajectory = test_xy_trajectories[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_eval = 2\n",
    "traj = eval_trajs[i_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'maze_dim': maze_dim,\n",
    "    'horizon': 200,\n",
    "    'state_dim': state_dim,\n",
    "    'node_encoding_corr': node_encoding_corr,\n",
    "    'initialization_seed': np.array(traj['initialization_seed']).item(),\n",
    "    'goal': np.array(traj['goal'])\n",
    "}\n",
    "env = DarkroomEnv(**env_config)\n",
    "env.goal = np.array(env.node_map_pos_to_encoding[tuple(test_xy_goal)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 1\n",
    "full_test_xy_trajectory = []\n",
    "\n",
    "for _ in range(repeats):\n",
    "    full_test_xy_trajectory.extend(deepcopy(test_xy_trajectory))\n",
    "    full_test_xy_trajectory.extend(deepcopy(test_xy_trajectory[:-1][::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = generate_trajectory_batch(full_test_xy_trajectory, test_xy_goal, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) to (0, 1) with action 1 and reward 0.0\n",
      "(0, 1) to (1, 1) with action 2 and reward 0.0\n",
      "(1, 1) to (2, 1) with action 2 and reward 0.0\n",
      "(2, 1) to (3, 1) with action 2 and reward 0.0\n",
      "(3, 1) to (4, 1) with action 2 and reward 0.0\n",
      "(4, 1) to (4, 2) with action 1 and reward 0.0\n",
      "(4, 2) to (3, 2) with action 0 and reward 0.0\n",
      "(3, 2) to (2, 2) with action 0 and reward 1.0\n",
      "(2, 2) to (3, 2) with action 2 and reward 0.0\n",
      "(3, 2) to (4, 2) with action 2 and reward 0.0\n",
      "(4, 2) to (4, 1) with action 3 and reward 0.0\n",
      "(4, 1) to (3, 1) with action 0 and reward 0.0\n",
      "(3, 1) to (2, 1) with action 0 and reward 0.0\n",
      "(2, 1) to (1, 1) with action 0 and reward 0.0\n",
      "(1, 1) to (0, 1) with action 0 and reward 0.0\n",
      "(0, 1) to (0, 0) with action 3 and reward 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch['context_states'].shape[1]):\n",
    "    start = batch['context_states'][:, i].cpu().numpy().squeeze()\n",
    "    next = batch['context_next_states'][:, i].cpu().numpy().squeeze()\n",
    "    action = np.argmax(batch['context_actions'][:, i].cpu().numpy())\n",
    "    reward = batch['context_rewards'][:, i].cpu().numpy().squeeze()\n",
    "    start = env.node_map_encoding_to_pos[tuple(start.tolist())]\n",
    "    next = env.node_map_encoding_to_pos[tuple(next.tolist())]\n",
    "    print(f'{start} to {next} with action {action} and reward {reward}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "curr_xy_state = full_test_xy_trajectory[-1]\n",
    "query_state = env.node_map_pos_to_encoding[tuple(curr_xy_state)]\n",
    "xy_path = [curr_xy_state]\n",
    "chosen_actions = []\n",
    "experienced_rewards = []\n",
    "\n",
    "# Convert initial context tensors to numpy for easier concatenation\n",
    "context_states = batch['context_states'].cpu().numpy().squeeze()\n",
    "context_actions = batch['context_actions'].cpu().numpy().squeeze()\n",
    "context_next_states = batch['context_next_states'].cpu().numpy().squeeze()\n",
    "context_rewards = batch['context_rewards'].cpu().numpy().squeeze()\n",
    "\n",
    "for i in range(n_steps):\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([context_states]),\n",
    "        'context_actions': convert_to_tensor([context_actions]),\n",
    "        'context_next_states': convert_to_tensor([context_next_states]),\n",
    "        'context_rewards': convert_to_tensor([context_rewards[:, None]]),\n",
    "        }\n",
    "    batch['query_states'] = convert_to_tensor([np.array(query_state)])\n",
    "    batch['query_states'] = batch['query_states'].to(model.device)\n",
    "    batch['zeros'] = torch.zeros(1, state_dim ** 2 + 5 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "\n",
    "    pred_action = torch.argmax(out.squeeze()).item()\n",
    "    action_encoding = np.zeros(5)\n",
    "    action_encoding[pred_action] = 1\n",
    "    next_state_encoding, reward = env.transit(np.array(query_state), action_encoding)\n",
    "\n",
    "    context_states = np.vstack([context_states, query_state])\n",
    "    context_actions = np.vstack([context_actions, action_encoding])\n",
    "    context_next_states = np.vstack([context_next_states, next_state_encoding])\n",
    "    context_rewards = np.append(context_rewards, reward)\n",
    "\n",
    "    xy_path.append(env.node_map_encoding_to_pos[tuple(next_state_encoding)])\n",
    "    chosen_actions.append(pred_action)\n",
    "    experienced_rewards.append(reward)\n",
    "    query_state = next_state_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (2, 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 142x142 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_trajectory(\n",
    "    env.to_networkx(),\n",
    "    observed_path=test_xy_trajectory,\n",
    "    taken_path=xy_path,\n",
    "    savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get chance performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance Level Performance: 6/256 = 0.02\n"
     ]
    }
   ],
   "source": [
    "# Eval Test 1\n",
    "shortest_path_len = 4\n",
    "n_paths = shortest_path_len**4\n",
    "x_diff = 2\n",
    "y_diff = 2\n",
    "n_shortest_paths = comb(shortest_path_len, x_diff)\n",
    "print(f\"Chance Level Performance: {n_shortest_paths}/{n_paths} = {n_shortest_paths/n_paths:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance Level Performance: 20/1296 = 0.02\n"
     ]
    }
   ],
   "source": [
    "# Eval Test 2\n",
    "shortest_path_len = 6\n",
    "n_paths = shortest_path_len**4\n",
    "x_diff = 3\n",
    "y_diff = 3\n",
    "n_shortest_paths = comb(shortest_path_len, x_diff)\n",
    "print(f\"Chance Level Performance: {n_shortest_paths}/{n_paths} = {n_shortest_paths/n_paths:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
