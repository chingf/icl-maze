{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "import random\n",
    "from src.evals.eval_trees import EvalTrees\n",
    "from src.evals.eval_trees import EvalCntrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=32-val_loss=0.000378.ckpt\n"
     ]
    }
   ],
   "source": [
    "corr = 0.0\n",
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"tree_maze\")\n",
    "fignum = '10'\n",
    "recalc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=33-val_loss=0.000662.ckpt\n"
     ]
    }
   ],
   "source": [
    "corr = 0.25\n",
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"tree_maze_bigger_models\", \"layer6\")\n",
    "fignum = '11'\n",
    "recalc_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "# Extract correlation and state_dim from eval dataset path\n",
    "state_dim = int(re.search(r'state_dim(\\d+)', eval_dset_path).group(1))\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": 10,\n",
    "    \"action_dim\": 4,\n",
    "    \"dropout\": dropout,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2431268/4170590101.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_pkl)\n"
     ]
    }
   ],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model_config['initialization_seed'] = 0\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_envs = 50\n",
    "\n",
    "is_h5_file = eval_dset_path.endswith('.h5')\n",
    "if is_h5_file:\n",
    "    eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    traj_indices = list(eval_trajs.keys())\n",
    "    n_eval_envs = min(n_eval_envs, len(traj_indices))\n",
    "    random.seed(0)\n",
    "    traj_indices = random.sample(traj_indices, n_eval_envs)\n",
    "    random.seed()\n",
    "    eval_trajs = [eval_trajs[i] for i in traj_indices]\n",
    "else:  # Pickle file\n",
    "    with open(eval_dset_path, 'rb') as f:\n",
    "        eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = min(n_eval_envs, len(eval_trajs))\n",
    "    random.seed(0)\n",
    "    eval_trajs = random.sample(eval_trajs, n_eval_envs)\n",
    "    random.seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transformer_input_from_batch(batch, model):\n",
    "    query_states = batch['query_states'][:, None, :]\n",
    "    zeros = batch['zeros'][:, None, :]\n",
    "    state_seq = torch.cat([batch['context_states'], query_states], dim=1)\n",
    "    action_seq = torch.cat(\n",
    "        [batch['context_actions'], zeros[:, :, :model.action_dim]], dim=1)\n",
    "    next_state_seq = torch.cat(\n",
    "        [batch['context_next_states'], zeros[:, :, :model.state_dim]], dim=1)\n",
    "    reward_seq = torch.cat([batch['context_rewards'], zeros[:, :, :1]], dim=1)\n",
    "    seq = torch.cat(\n",
    "        [state_seq, action_seq, next_state_seq, reward_seq], dim=2)\n",
    "    seq_len = seq.shape[1]\n",
    "    stacked_inputs = model.embed_transition(seq)\n",
    "    return stacked_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batches(traj, context_length=1000):\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([np.array(traj['context_states'][:context_length])]),\n",
    "        'context_actions': convert_to_tensor([np.array(traj['context_actions'][:context_length])]),\n",
    "        'context_next_states': convert_to_tensor([np.array(traj['context_next_states'][:context_length])]),\n",
    "        'context_rewards': convert_to_tensor([np.array(traj['context_rewards'][:context_length])[:, None]]),\n",
    "        'query_states': convert_to_tensor([np.array(traj['query_state'])]),\n",
    "        } \n",
    "    batch['zeros'] = torch.zeros(1, 10 ** 2 + 4 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        if 'context' in k:\n",
    "            batch[k] = batch[k]\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "    baseline_batch = {}\n",
    "    for k, v in batch.items():\n",
    "        baseline_batch[k] = v.clone() if isinstance(v, torch.Tensor) else v\n",
    "    baseline_batch['context_actions'] *= 0\n",
    "    baseline_batch['context_actions'] += 0.25\n",
    "    return baseline_batch, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_token_attributions(attributions, aggregation_method='l2'):\n",
    "    \"\"\"\n",
    "    Aggregate attributions across embedding dimensions to get token-level scores.\n",
    "    \n",
    "    Args:\n",
    "        attributions: Attribution tensor of shape [seq_len, emb_dim]\n",
    "        aggregation_method: Method to aggregate across embedding dimensions ('sum', 'l2', 'mean')\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of token-level attributions of shape [seq_len]\n",
    "    \"\"\"\n",
    "    if aggregation_method == 'sum':\n",
    "        return torch.sum(attributions, dim=-1)\n",
    "    elif aggregation_method == 'l2':\n",
    "        return torch.sqrt(torch.sum(attributions ** 2, dim=-1))\n",
    "    elif aggregation_method == 'mean':\n",
    "        return torch.mean(attributions, dim=-1)\n",
    "    elif aggregation_method == 'sum-abs':\n",
    "        return torch.sum(torch.abs(attributions), dim=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Gradient Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_attributions(traj, model):\n",
    "    baseline_batch, batch = format_batches(traj)\n",
    "    layer_attributions = {}\n",
    "    inputs = format_transformer_input_from_batch(batch, model)\n",
    "    baseline_inputs = format_transformer_input_from_batch(baseline_batch, model)\n",
    "\n",
    "    # Output w.r.t Input\n",
    "    alphas = torch.linspace(0, 1, steps=20)\n",
    "    all_grads = []\n",
    "    for alpha in alphas:\n",
    "        interp_input = alpha*inputs + (1-alpha)*baseline_inputs\n",
    "        interp_input.requires_grad_(True)\n",
    "\n",
    "        # Forward pass to get layer output\n",
    "        transformer_output = model.transformer(inputs_embeds=interp_input)\n",
    "        preds = model.pred_actions(transformer_output['last_hidden_state'])\n",
    "        preds = preds[:, -1, :]\n",
    "        target = preds[:, traj['optimal_action'].argmax()]\n",
    "\n",
    "        grad_wrt_input = torch.autograd.grad(\n",
    "            outputs=target,\n",
    "            inputs=interp_input,\n",
    "            grad_outputs=target,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        all_grads.append(grad_wrt_input)\n",
    "    avg_grad = torch.stack(all_grads).mean(dim=0)\n",
    "    avg_grad = avg_grad.detach().cpu().numpy().squeeze()\n",
    "    delta_input = (inputs - baseline_inputs).detach().cpu().numpy().squeeze()\n",
    "    input_to_output_attribution = np.sum(avg_grad * delta_input, axis=1)\n",
    "    return input_to_output_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states_on_path(query_state, env, optimal_action_map):\n",
    "    opt_act = -1\n",
    "    curr_state = query_state\n",
    "    states_on_path = []\n",
    "    while opt_act != 3:\n",
    "        states_on_path.append(curr_state)\n",
    "        opt_act = optimal_action_map[tuple(curr_state)]\n",
    "        act_vector = np.zeros(4)\n",
    "        act_vector[opt_act] = 1\n",
    "        curr_state = env.transit(curr_state, act_vector)[0]\n",
    "    return states_on_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtree_location(layer, pos, subtree):\n",
    "    midpt = 2**(layer-1)\n",
    "    quarter_pt = midpt//2\n",
    "    eighth_pt = quarter_pt//2\n",
    "    if layer == 0:\n",
    "        return 0\n",
    "    if subtree == 'half':\n",
    "        return 1 if pos < midpt else 2\n",
    "    elif subtree == 'quarter':\n",
    "        if layer == 1:\n",
    "            return 0\n",
    "        bins = np.arange(0, 2**layer, quarter_pt)\n",
    "        return np.digitize([pos], bins)[0]\n",
    "    elif subtree == 'eighth':\n",
    "        if (layer == 1) or (layer == 2):\n",
    "            return 0\n",
    "        bins = np.arange(0, 2**layer, eighth_pt)\n",
    "        return np.digitize([pos], bins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer1_parent(node):\n",
    "    if node.layer == 1:\n",
    "        return node.encoding()\n",
    "    else:\n",
    "        return get_layer1_parent(node.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'grad_val': [],\n",
    "    'node_dist_from_goal': [],\n",
    "    'query_dist_from_goal': [],\n",
    "    'query_is_same_quarter_as_goal': [],\n",
    "}\n",
    "results_B = {\n",
    "    'grad_val': [],\n",
    "    'node_is_same_quarter_as_goal': [],\n",
    "    'node_dist_from_goal': [],\n",
    "    'node_dist_from_query': [],\n",
    "    'query_is_same_quarter_as_goal': [],\n",
    "    'query_dist_from_goal': [],\n",
    "}\n",
    "n_iters = 2\n",
    "\n",
    "for i_eval in range(n_eval_envs):\n",
    "    traj = eval_trajs[i_eval]\n",
    "    first_reward_idx = np.where(traj['context_rewards'] != 0)[0]\n",
    "    if (first_reward_idx.size == 0) or (first_reward_idx[0] > 800):\n",
    "        continue\n",
    "    env_config = {\n",
    "        'max_layers': 7,\n",
    "        'horizon': 1600,\n",
    "        'branching_prob': 1.0,\n",
    "        'node_encoding_corr': corr,\n",
    "        'state_dim': state_dim,\n",
    "        'initialization_seed': np.array(traj['initialization_seed']).item()\n",
    "    }\n",
    "    env = EvalCntrees().create_env(env_config, np.array(traj['goal']), i_eval)\n",
    "    optimal_action_map, dist_from_goal = env.make_opt_action_dict()\n",
    "    for query_dist_from_goal in range(1, 13):\n",
    "        for _ in range(n_iters):\n",
    "            valid_query_states = []\n",
    "            for i in range(len(traj['context_states'])):\n",
    "                if traj['context_states'][i].tolist() == list(env.root.encoding()):\n",
    "                    continue\n",
    "                d = dist_from_goal[tuple(traj['context_states'][i].tolist())]\n",
    "                if d != query_dist_from_goal:\n",
    "                    continue\n",
    "                valid_query_states.append(traj['context_states'][i])\n",
    "            if len(valid_query_states) == 0:\n",
    "                continue\n",
    "            traj['query_state'] = valid_query_states[np.random.choice(len(valid_query_states))]\n",
    "            query_state = traj['query_state'].tolist()\n",
    "            goal_state = traj['goal'].tolist()\n",
    "            query_layer = env.node_map[tuple(query_state)].layer\n",
    "            query_pos = env.node_map[tuple(query_state)].pos\n",
    "            goal_layer = env.node_map[tuple(goal_state)].layer\n",
    "            goal_pos = env.node_map[tuple(goal_state)].pos\n",
    "            _, dist_from_query = env._make_opt_action_dict(tuple(query_state))\n",
    "            root_state = list(env.root.encoding())\n",
    "            layer1_parent_state = list(get_layer1_parent(env.node_map[tuple(query_state)]))\n",
    "            same_quarter_as_goal = get_subtree_location(query_layer, query_pos, 'quarter') == get_subtree_location(goal_layer, goal_pos, 'quarter')\n",
    "            states_on_path = get_states_on_path(query_state, env, optimal_action_map)\n",
    "            attentions = get_output_attributions(traj, model)\n",
    "            max_scale = np.abs(attentions).max()\n",
    "            attentions = np.abs(attentions)/max_scale\n",
    "    \n",
    "            for idx, attn in enumerate(attentions):\n",
    "                curr_state = traj['context_states'][idx].tolist()\n",
    "                next_state = traj['context_next_states'][idx].tolist()\n",
    "                if curr_state == next_state:\n",
    "                    continue\n",
    "                curr_state_layer = env.node_map[tuple(curr_state)].layer\n",
    "                curr_state_pos = env.node_map[tuple(curr_state)].pos\n",
    "                next_state_layer = env.node_map[tuple(next_state)].layer\n",
    "                next_state_pos = env.node_map[tuple(next_state)].pos\n",
    "                curr_state_is_same_quarter_as_goal = \\\n",
    "                    get_subtree_location(curr_state_layer, curr_state_pos, 'quarter') == get_subtree_location(goal_layer, goal_pos, 'quarter') or \\\n",
    "                    curr_state == root_state or \\\n",
    "                    curr_state == layer1_parent_state\n",
    "                next_state_is_same_quarter_as_goal = \\\n",
    "                    get_subtree_location(next_state_layer, next_state_pos, 'quarter') == get_subtree_location(goal_layer, goal_pos, 'quarter') or \\\n",
    "                    next_state == root_state or \\\n",
    "                    next_state == layer1_parent_state\n",
    "                results_B['grad_val'].append(attn)\n",
    "                results_B['node_is_same_quarter_as_goal'].append(curr_state_is_same_quarter_as_goal or next_state_is_same_quarter_as_goal)\n",
    "                results_B['node_dist_from_goal'].append(np.min([dist_from_goal[tuple(curr_state)], dist_from_goal[tuple(next_state)]]))\n",
    "                results_B['node_dist_from_query'].append(np.min([dist_from_query[tuple(curr_state)], dist_from_query[tuple(next_state)]]))\n",
    "                results_B['query_is_same_quarter_as_goal'].append(same_quarter_as_goal)\n",
    "                results_B['query_dist_from_goal'].append(query_dist_from_goal)\n",
    "                if curr_state in states_on_path and next_state in states_on_path:\n",
    "                    results['node_dist_from_goal'].append(np.min([dist_from_goal[tuple(curr_state)], dist_from_goal[tuple(next_state)]]))\n",
    "                    results['query_dist_from_goal'].append(query_dist_from_goal)\n",
    "                    results['grad_val'].append(attn)\n",
    "                    results['query_is_same_quarter_as_goal'].append(same_quarter_as_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results_B)\n",
    "df['grad_val'] = np.abs(df['grad_val'])\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "n_colors = len(df['query_dist_from_goal'].unique())\n",
    "custom_blues = sns.color_palette(\"Blues\", n_colors=n_colors*2+1)[2::2] # Skip the lightest colors\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df, x='node_dist_from_query', y='grad_val',\n",
    "    legend=False, linewidth=2,\n",
    ")\n",
    "plt.xlabel('Dist. from Query', fontsize=8)\n",
    "plt.ylabel('Attrib.', fontsize=8)\n",
    "plt.xticks(fontsize=6)\n",
    "plt.yticks(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figs_app/C_{fignum}D.png', transparent=True, dpi=300)\n",
    "plt.savefig(f'figs_app/C_{fignum}D.pdf', transparent=True, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame(results_B)\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "n_colors = len(df['query_dist_from_goal'].unique())\n",
    "custom_blues = sns.color_palette(\"Blues\", n_colors=n_colors*2+1)[2::2] # Skip the lightest colors\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df, x='node_dist_from_goal', y='grad_val',\n",
    "    legend=False, linewidth=2,\n",
    ")\n",
    "plt.xlabel('Dist. from Goal', fontsize=8)\n",
    "plt.ylabel('Attrib.', fontsize=8)\n",
    "plt.xticks(fontsize=6)\n",
    "plt.yticks(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figs_app/C_{fignum}E.png', transparent=True, dpi=300)\n",
    "plt.savefig(f'figs_app/C_{fignum}E.pdf', transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
