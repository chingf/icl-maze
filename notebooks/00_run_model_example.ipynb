{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "import random\n",
    "from src.evals.eval_trees import EvalTrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = \"/n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/\"\n",
    "wandb_project = \"cntree\"\n",
    "corr = 0.25\n",
    "env_name = f\"cntree_layers7_bprob0.9_corr{corr}_state_dim10_envs300000_H800_explore\"\n",
    "model_name = \"transformer_end_query_embd512_layer3_head4_lr0.0001_drop0.2_batch256\"\n",
    "model_path = os.path.join(engram_dir, wandb_project, env_name, \"models\", model_name)\n",
    "ckpt_name = find_ckpt_file(model_path, \"best\")\n",
    "print(ckpt_name)\n",
    "path_to_pkl = os.path.join(model_path, ckpt_name)\n",
    "\n",
    "eval_dset_path = f\"/n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/cntree/cntree_layers7_bprob1.0_corr{corr}_state_dim10_envs1000_H1600_explore/datasets/eval.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": 10,\n",
    "    \"action_dim\": 4,\n",
    "    \"dropout\": dropout,\n",
    "    \"train_on_last_pred_only\": False,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "    \"linear_attention\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_dataset_path = \"/n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/lazyload/tree_layers7_bprob1.0_envs600000_H1600_explore/datasets/eval.h5\"\n",
    "#eval_dset_path = \"/n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/random_tree/tree_layers7_bprob0.9_envs300000_H800_explore/datasets/eval.pkl\"\n",
    "eval_dset_path = \"/n/holylfs06/LABS/krajan_lab/Lab/cfang/icl-maze/datasets/tree_layers7_bprob0.9_envs300000_H800_explore/eval.pkl\"\n",
    "n_eval_envs = 2000\n",
    "\n",
    "is_h5_file = eval_dset_path.endswith('.h5')\n",
    "if is_h5_file:\n",
    "    eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    traj_indices = list(eval_trajs.keys())\n",
    "    n_eval_envs = min(n_eval_envs, len(traj_indices))\n",
    "    random.seed(0)\n",
    "    traj_indices = random.sample(traj_indices, n_eval_envs)\n",
    "    random.seed()\n",
    "    eval_trajs = [eval_trajs[i] for i in traj_indices]\n",
    "else:  # Pickle file\n",
    "    with open(eval_dset_path, 'rb') as f:\n",
    "        eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = min(n_eval_envs, len(eval_trajs))\n",
    "    random.seed(0)\n",
    "    eval_trajs = random.sample(eval_trajs, n_eval_envs)\n",
    "    random.seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "optimal_actions = []\n",
    "rs = []\n",
    "for i_eval in range(n_eval_envs):\n",
    "    traj = eval_trajs[i_eval]\n",
    "    env_config = {\n",
    "        'max_layers': 7,\n",
    "        'horizon': 800,\n",
    "        'branching_prob': 0.9,\n",
    "        'node_encoding': 'random',\n",
    "        'initialization_seed': np.array(traj['initialization_seed']).item()\n",
    "    }\n",
    "    #env = EvalTrees().create_env(env_config, np.array(traj['goal']), i_eval)\n",
    "\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([np.array(traj['context_states'])]),\n",
    "        'context_actions': convert_to_tensor([np.array(traj['context_actions'])]),\n",
    "        'context_next_states': convert_to_tensor([np.array(traj['context_next_states'])]),\n",
    "        'context_rewards': convert_to_tensor([np.array(traj['context_rewards'])[:, None]]),\n",
    "        'query_states': convert_to_tensor([np.array(traj['query_state'])]),\n",
    "        }\n",
    "    rs.append(batch['context_rewards'].sum().item())\n",
    "\n",
    "    #assert env.root.encoding_vector == tuple(traj['context_states'][0])\n",
    "    continue\n",
    "\n",
    "\n",
    "    batch['zeros'] = torch.zeros(1, 10 ** 2 + 4 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        if 'context' in k:\n",
    "            batch[k] = batch[k]\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "    print(out)\n",
    "\n",
    "    # Get predicted and optimal actions\n",
    "    pred_action = torch.argmax(out.squeeze()).item()\n",
    "    optimal_action = np.argmax(traj['optimal_action'])\n",
    "\n",
    "    print(f\"\\nPredicted action: {pred_action}\")\n",
    "    print(f\"Optimal action: {optimal_action}\")\n",
    "    print(f\"Match: {pred_action == optimal_action}\")\n",
    "    matches.append(pred_action == optimal_action)\n",
    "    optimal_actions.append(optimal_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(rs)==0).sum()/len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(optimal_actions)==0).sum()/len(optimal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(matches).sum()/len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
