{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "from src.evals.eval_trees import EvalCntrees\n",
    "from decoding_utils import fit_and_evaluate_classification, fit_and_evaluate_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 0.25\n",
    "seq_length = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=34-val_loss=0.174876.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"tree_maze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "# Extract correlation and state_dim from eval dataset path\n",
    "state_dim = int(re.search(r'state_dim(\\d+)', eval_dset_path).group(1))\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": 10,\n",
    "    \"action_dim\": 4,\n",
    "    \"dropout\": dropout,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model_config['initialization_seed'] = 0\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_trajs(eval_dset_path):\n",
    "    is_h5_file = eval_dset_path.endswith('.h5')\n",
    "    if is_h5_file:\n",
    "        eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    else:  # Pickle file\n",
    "        with open(eval_dset_path, 'rb') as f:\n",
    "            eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = len(eval_trajs)\n",
    "    return eval_trajs, n_eval_envs\n",
    "\n",
    "eval_trajs_1, n_eval_envs_1 = load_eval_trajs(eval_dset_path)\n",
    "eval_trajs_2, n_eval_envs_2 = load_eval_trajs(eval_dset_path.replace('eval', 'test'))\n",
    "eval_trajs = eval_trajs_1 + eval_trajs_2\n",
    "n_eval_envs = n_eval_envs_1 + n_eval_envs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(traj, model, seq_length=1200, start_idx=800):\n",
    "    hidden_states = []\n",
    "\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([np.array(traj['context_states'])]),\n",
    "        'context_actions': convert_to_tensor([np.array(traj['context_actions'])]),\n",
    "        'context_next_states': convert_to_tensor([np.array(traj['context_next_states'])]),\n",
    "        'context_rewards': convert_to_tensor([np.array(traj['context_rewards'])[:, None]]),\n",
    "        'query_states': convert_to_tensor([np.array(traj['query_state'])]),  # Ignored\n",
    "        }\n",
    "    batch['zeros'] = torch.zeros(1, 10 ** 2 + 4 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        if 'context' in k:\n",
    "            batch[k] = batch[k][:,:seq_length]\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "    model.save_activations = True\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "    _hidden_states = model.activations['hidden_states'] # Tuple over layers of (1, seq, dim)\n",
    "    state_features = batch['context_states'][0][start_idx:].to('cpu').numpy()\n",
    "    next_state_features = batch['context_next_states'][0][start_idx:].to('cpu').numpy()\n",
    "    actions = batch['context_actions'][0].argmax(dim=1)[start_idx:].to('cpu').numpy()\n",
    "    for i_layer in range(len(_hidden_states)):\n",
    "        hidden_states.append(_hidden_states[i_layer][0,start_idx:-1])\n",
    "    return hidden_states, state_features, next_state_features, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Across context decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtree_location(layer, pos, subtree):\n",
    "    midpt = 2**(layer-1)\n",
    "    quarter_pt = midpt//2\n",
    "    eighth_pt = quarter_pt//2\n",
    "    if layer == 0:\n",
    "        return 0\n",
    "    if subtree == 'half':\n",
    "        return 1 if pos < midpt else 2\n",
    "    elif subtree == 'quarter':\n",
    "        if layer == 1:\n",
    "            return 0\n",
    "        bins = np.arange(0, 2**layer, quarter_pt)\n",
    "        return np.digitize([pos], bins)[0]\n",
    "    elif subtree == 'eighth':\n",
    "        if (layer == 1) or (layer == 2):\n",
    "            return 0\n",
    "        bins = np.arange(0, 2**layer, eighth_pt)\n",
    "        return np.digitize([pos], bins)[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_matrices():\n",
    "    train_envs = np.arange(1, int(n_eval_envs*0.9))\n",
    "    test_envs = np.arange(int(n_eval_envs*0.9), n_eval_envs)\n",
    "\n",
    "    X_train = [[] for _ in range(model.n_layer+1)]\n",
    "    X_test = [[] for _ in range(model.n_layer+1)]\n",
    "    Ys_dict = {\n",
    "        \"dist_from_goal\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"layer\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"node_identity\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"maze_half\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"maze_quarter\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"maze_eighth\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        'is_goal': {\"Y_train\": [], \"Y_test\": []},\n",
    "        'same_half_as_goal': {\"Y_train\": [], \"Y_test\": []},\n",
    "        'same_quarter_as_goal': {\"Y_train\": [], \"Y_test\": []},\n",
    "        'opt_action': {\"Y_train\": [], \"Y_test\": []},\n",
    "        'state_feature': {\"Y_train\": [], \"Y_test\": []},\n",
    "        'next_state_feature': {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"on_path\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"on_lr_path\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"intersects_lr_path\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"inverse_action\": {\"Y_train\": [], \"Y_test\": []},\n",
    "        \"action\": {\"Y_train\": [], \"Y_test\": []}\n",
    "    }\n",
    "    for i_eval in range(n_eval_envs):\n",
    "        traj = eval_trajs[i_eval]\n",
    "        first_reward = np.argwhere(np.array(traj['context_rewards'])>0)\n",
    "        if (first_reward.size == 0) or (first_reward[0] > seq_length):\n",
    "            continue\n",
    "        start_idx = first_reward[0].item()\n",
    "\n",
    "        env_config = {\n",
    "            'max_layers': 7,\n",
    "            'horizon': 1600,\n",
    "            'branching_prob': 1.0,\n",
    "            'node_encoding_corr': corr,\n",
    "            'state_dim': state_dim,\n",
    "            'initialization_seed': np.array(traj['initialization_seed']).item()\n",
    "        }\n",
    "        env = EvalCntrees().create_env(env_config, np.array(traj['goal']), i_eval)\n",
    "        opt_action_map, dist_from_goal = env.make_opt_action_dict()\n",
    "        s = env.root.encoding()\n",
    "        states_on_path_from_root_to_goal = [s]\n",
    "        while True:\n",
    "            action = np.zeros(4)\n",
    "            action[opt_action_map[tuple(s)]] = 1\n",
    "            s, _ = env.transit(np.array(s), action)\n",
    "            states_on_path_from_root_to_goal.append(tuple(s))\n",
    "            if np.array_equal(s, env.goal):\n",
    "                break\n",
    "\n",
    "        hidden_states, state_features, next_state_features, actions = run_model(traj, model, seq_length, start_idx)\n",
    "        goal_node = env.node_map[tuple(env.goal.tolist())]\n",
    "        goal_layer = goal_node.layer\n",
    "        goal_pos = goal_node.pos\n",
    "        seen_combos = set()\n",
    "        for state_idx in reversed(range(len(state_features))):\n",
    "            state_feature = state_features[state_idx]\n",
    "            next_state_feature = next_state_features[state_idx]\n",
    "            state_feature_tuple = tuple(state_feature.tolist())\n",
    "            next_state_feature_tuple = tuple(next_state_feature.tolist())\n",
    "            action = actions[state_idx]\n",
    "            combo = tuple(state_feature.tolist() + next_state_feature.tolist())\n",
    "            if combo in seen_combos:\n",
    "                continue\n",
    "            seen_combos.add(combo)\n",
    "            d = dist_from_goal[next_state_feature_tuple]\n",
    "            layer = env.node_map[next_state_feature_tuple].layer\n",
    "            pos = env.node_map[next_state_feature_tuple].pos\n",
    "            node_identity = 2**layer + pos\n",
    "            maze_half = get_subtree_location(layer, pos, 'half')\n",
    "            maze_quarter = get_subtree_location(layer, pos, 'quarter')\n",
    "            maze_eighth = get_subtree_location(layer, pos, 'eighth')\n",
    "            if action == 0:\n",
    "                if env.node_map[next_state_feature_tuple].left == env.node_map[state_feature_tuple]:\n",
    "                    inverse_action = 1\n",
    "                elif env.node_map[next_state_feature_tuple].right == env.node_map[state_feature_tuple]:\n",
    "                    inverse_action = 2\n",
    "                else:\n",
    "                    inverse_action = -1\n",
    "            elif action == 1 or action == 2:\n",
    "                inverse_action = 0\n",
    "            else:\n",
    "                inverse_action = 3\n",
    "\n",
    "            on_lr_path = (state_feature_tuple in states_on_path_from_root_to_goal) and (next_state_feature_tuple in states_on_path_from_root_to_goal)\n",
    "            intersects_lr_path = (state_feature_tuple in states_on_path_from_root_to_goal) or (next_state_feature_tuple in states_on_path_from_root_to_goal)\n",
    "            \n",
    "            Y_key = \"Y_train\" if i_eval in train_envs else \"Y_test\"\n",
    "            Ys_dict[\"dist_from_goal\"][Y_key].append(d)\n",
    "            Ys_dict[\"layer\"][Y_key].append(layer)\n",
    "            Ys_dict[\"node_identity\"][Y_key].append(node_identity)\n",
    "            Ys_dict[\"maze_half\"][Y_key].append(maze_half)\n",
    "            Ys_dict[\"maze_quarter\"][Y_key].append(maze_quarter)\n",
    "            Ys_dict[\"maze_eighth\"][Y_key].append(maze_eighth)\n",
    "            Ys_dict[\"is_goal\"][Y_key].append(state_feature_tuple == tuple(env.goal.tolist()))\n",
    "            Ys_dict[\"same_half_as_goal\"][Y_key].append(maze_half == get_subtree_location(goal_layer, goal_pos, 'half'))\n",
    "            Ys_dict[\"same_quarter_as_goal\"][Y_key].append(maze_quarter == get_subtree_location(goal_layer, goal_pos, 'quarter'))\n",
    "            Ys_dict[\"opt_action\"][Y_key].append(opt_action_map[state_feature_tuple])\n",
    "            Ys_dict[\"state_feature\"][Y_key].append(state_feature)\n",
    "            Ys_dict[\"next_state_feature\"][Y_key].append(next_state_feature)\n",
    "            Ys_dict[\"on_path\"][Y_key].append(action == opt_action_map[state_feature_tuple])\n",
    "            Ys_dict[\"on_lr_path\"][Y_key].append(on_lr_path)\n",
    "            Ys_dict[\"intersects_lr_path\"][Y_key].append(intersects_lr_path)\n",
    "            Ys_dict[\"inverse_action\"][Y_key].append(inverse_action)\n",
    "            Ys_dict[\"action\"][Y_key].append(action)\n",
    "            \n",
    "            for layer in range(len(hidden_states)):\n",
    "                hidden_state = hidden_states[layer][state_idx].to('cpu').numpy()\n",
    "                if i_eval in train_envs:\n",
    "                    X_train[layer].append(hidden_state)\n",
    "                else:\n",
    "                    X_test[layer].append(hidden_state)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return X_train, X_test, Ys_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data and run regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Ys_dict = make_train_test_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.5868448098663926), np.float64(0.7929085303186023), np.float64(0.9275436793422405), np.float64(0.9696813977389517)]\n"
     ]
    }
   ],
   "source": [
    "pipeline, test_score, test_y, test_pred = fit_and_evaluate_classification(\n",
    "    X_train, Ys_dict['on_lr_path'][\"Y_train\"],\n",
    "    X_test, Ys_dict['on_lr_path'][\"Y_test\"], print_scores=False)\n",
    "\n",
    "print([np.mean(_test_score) for _test_score in test_score])\n",
    "\n",
    "results = {}\n",
    "results['test_score'] = test_score\n",
    "results['test_y'] = test_y\n",
    "results['test_pred'] = test_pred\n",
    "results['dist_from_goal'] = Ys_dict['dist_from_goal'][\"Y_test\"]\n",
    "with open('pickles/09_buffer_token_decoding_on_lr_path.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.4912536443148688), np.float64(0.8498542274052479), np.float64(0.9927113702623906), np.float64(0.9927113702623906)]\n"
     ]
    }
   ],
   "source": [
    "train_indices = [i for i, a in enumerate(Ys_dict['inverse_action']['Y_train']) if a in [1,2]]\n",
    "test_indices = [i for i, a in enumerate(Ys_dict['inverse_action']['Y_test']) if a in [1,2]]\n",
    "_Y_train = [Ys_dict['inverse_action'][\"Y_train\"][i] for i in train_indices]\n",
    "_Y_test = [Ys_dict['inverse_action'][\"Y_test\"][i] for i in test_indices]\n",
    "_X_train = [[X[i] for i in train_indices] for X in X_train] \n",
    "_X_test = [[X[i] for i in test_indices] for X in X_test]\n",
    "\n",
    "pipeline, test_score, test_y, test_pred = fit_and_evaluate_classification(\n",
    "    _X_train, _Y_train, _X_test, _Y_test,\n",
    "    print_scores=False\n",
    ")\n",
    "\n",
    "print([np.mean(_test_score) for _test_score in test_score])\n",
    "\n",
    "results = {}\n",
    "results['test_score'] = test_score\n",
    "results['test_y'] = test_y\n",
    "results['test_pred'] = test_pred\n",
    "results['dist_from_goal'] = [Ys_dict['dist_from_goal'][\"Y_test\"][i] for i in test_indices]\n",
    "with open('pickles/09_buffer_token_decoding_inverse_action.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
