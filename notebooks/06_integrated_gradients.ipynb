{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import configs\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from src.utils import find_ckpt_file, convert_to_tensor\n",
    "import h5py\n",
    "import random\n",
    "from src.evals.eval_trees import EvalTrees\n",
    "from src.evals.eval_trees import EvalCntrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=34-val_loss=0.174876.ckpt\n"
     ]
    }
   ],
   "source": [
    "corr = 0.25\n",
    "model_name, path_to_pkl, eval_dset_path = configs.get_model_paths(corr, \"tree_maze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters using regex\n",
    "import re\n",
    "\n",
    "n_embd = int(re.search(r'embd(\\d+)', model_name).group(1))\n",
    "n_layer = int(re.search(r'layer(\\d+)', model_name).group(1))\n",
    "n_head = int(re.search(r'head(\\d+)', model_name).group(1))\n",
    "dropout = float(re.search(r'drop(\\d*\\.?\\d*)', model_name).group(1))\n",
    "\n",
    "# Extract correlation and state_dim from eval dataset path\n",
    "state_dim = int(re.search(r'state_dim(\\d+)', eval_dset_path).group(1))\n",
    "\n",
    "model_config = {\n",
    "    \"n_embd\": n_embd,\n",
    "    \"n_layer\": n_layer,\n",
    "    \"n_head\": n_head,\n",
    "    \"state_dim\": 10,\n",
    "    \"action_dim\": 4,\n",
    "    \"dropout\": dropout,\n",
    "    \"test\": True,\n",
    "    \"name\": \"transformer_end_query\",\n",
    "    \"optimizer_config\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3181195/4170590101.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path_to_pkl)\n"
     ]
    }
   ],
   "source": [
    "from src.models.transformer_end_query import Transformer\n",
    "model_config['initialization_seed'] = 0\n",
    "model = Transformer(**model_config)\n",
    "checkpoint = torch.load(path_to_pkl)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_envs = 50\n",
    "\n",
    "is_h5_file = eval_dset_path.endswith('.h5')\n",
    "if is_h5_file:\n",
    "    eval_trajs = h5py.File(eval_dset_path, 'r')\n",
    "    traj_indices = list(eval_trajs.keys())\n",
    "    n_eval_envs = min(n_eval_envs, len(traj_indices))\n",
    "    random.seed(0)\n",
    "    traj_indices = random.sample(traj_indices, n_eval_envs)\n",
    "    random.seed()\n",
    "    eval_trajs = [eval_trajs[i] for i in traj_indices]\n",
    "else:  # Pickle file\n",
    "    with open(eval_dset_path, 'rb') as f:\n",
    "        eval_trajs = pickle.load(f)\n",
    "    n_eval_envs = min(n_eval_envs, len(eval_trajs))\n",
    "    random.seed(0)\n",
    "    eval_trajs = random.sample(eval_trajs, n_eval_envs)\n",
    "    random.seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [  # (env, seed, figname, query_type)\n",
    "    (1, 1, 'figs/6a_1', 'far'),\n",
    "    (2, 3, 'figs/6a_2', 'far'),\n",
    "    (4, 0, 'figs_app/L_20E_1', 'far'),\n",
    "    (5, 0, 'figs_app/L_20E_2', 'far'),\n",
    "    (6, 0, 'figs_app/L_20E_3', 'far'),\n",
    "    (8, 0, 'figs_app/L_20E_4', 'far'),\n",
    "]\n",
    "example_idx = 5\n",
    "i_eval = examples[example_idx][0]\n",
    "np.random.seed(examples[example_idx][1])\n",
    "random.seed(examples[example_idx][1])\n",
    "figname = examples[example_idx][2]\n",
    "query_type = examples[example_idx][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = eval_trajs[i_eval]\n",
    "env_config = {\n",
    "    'max_layers': 7,\n",
    "    'horizon': 1600,\n",
    "    'branching_prob': 1.0,\n",
    "    'node_encoding_corr': corr,\n",
    "    'state_dim': state_dim,\n",
    "    'initialization_seed': np.array(traj['initialization_seed']).item()\n",
    "}\n",
    "env = EvalCntrees().create_env(env_config, np.array(traj['goal']), i_eval)\n",
    "optimal_action_map, dist_from_goal = env.make_opt_action_dict()\n",
    "valid_query_states = []\n",
    "if query_type == 'root':\n",
    "    valid_query_states = [np.array(env.root.encoding())]\n",
    "elif query_type in ['far', 'middle']:\n",
    "    for i in range(len(traj['context_states'])):\n",
    "        if traj['context_states'][i].tolist() == list(env.root.encoding()):\n",
    "            continue\n",
    "        d = dist_from_goal[tuple(traj['context_states'][i].tolist())]\n",
    "        if query_type == 'far' and d < 12:\n",
    "            continue\n",
    "        elif query_type == 'middle' and d != 6:\n",
    "           continue\n",
    "        valid_query_states.append(traj['context_states'][i])\n",
    "if len(valid_query_states) == 0:\n",
    "    print('No valid query states found')\n",
    "else:\n",
    "    traj['query_state'] = valid_query_states[np.random.choice(len(valid_query_states))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 300, Reward: 5\n",
      "Context length: 600, Reward: 9\n",
      "Context length: 900, Reward: 9\n",
      "First nonzero reward at step 32: 1\n"
     ]
    }
   ],
   "source": [
    "for seq_length in [300, 600, 900]:\n",
    "    print(f'Context length: {seq_length}, Reward: {traj['context_rewards'][:seq_length].sum()}')\n",
    "\n",
    "first_reward_idx = np.where(traj['context_rewards'] != 0)[0][0]\n",
    "print(f'First nonzero reward at step {first_reward_idx}: {traj['context_rewards'][first_reward_idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Gradient Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 10)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj['context_states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transformer_input_from_batch(batch, model):\n",
    "    query_states = batch['query_states'][:, None, :]\n",
    "    zeros = batch['zeros'][:, None, :]\n",
    "    state_seq = torch.cat([batch['context_states'], query_states], dim=1)\n",
    "    action_seq = torch.cat(\n",
    "        [batch['context_actions'], zeros[:, :, :model.action_dim]], dim=1)\n",
    "    next_state_seq = torch.cat(\n",
    "        [batch['context_next_states'], zeros[:, :, :model.state_dim]], dim=1)\n",
    "    reward_seq = torch.cat([batch['context_rewards'], zeros[:, :, :1]], dim=1)\n",
    "    seq = torch.cat(\n",
    "        [state_seq, action_seq, next_state_seq, reward_seq], dim=2)\n",
    "    seq_len = seq.shape[1]\n",
    "    stacked_inputs = model.embed_transition(seq)\n",
    "    return stacked_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batches(traj):\n",
    "    batch = {\n",
    "        'context_states': convert_to_tensor([np.array(traj['context_states'][:context_length])]),\n",
    "        'context_actions': convert_to_tensor([np.array(traj['context_actions'][:context_length])]),\n",
    "        'context_next_states': convert_to_tensor([np.array(traj['context_next_states'][:context_length])]),\n",
    "        'context_rewards': convert_to_tensor([np.array(traj['context_rewards'][:context_length])[:, None]]),\n",
    "        'query_states': convert_to_tensor([np.array(traj['query_state'])]),\n",
    "        } \n",
    "    batch['zeros'] = torch.zeros(1, 10 ** 2 + 4 + 1).float()\n",
    "    for k in batch.keys():\n",
    "        if 'context' in k:\n",
    "            batch[k] = batch[k]\n",
    "        batch[k] = batch[k].to(model.device)\n",
    "    baseline_batch = {}\n",
    "    for k, v in batch.items():\n",
    "        baseline_batch[k] = v.clone() if isinstance(v, torch.Tensor) else v\n",
    "    baseline_batch['context_actions'] *= 0\n",
    "    #baseline_batch['context_actions'] += 0.25\n",
    "    return baseline_batch, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_batch, batch = format_batches(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = {}\n",
    "hooks = []\n",
    "\n",
    "def get_hook(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        layer_outputs[layer_name] = output\n",
    "    return hook\n",
    "\n",
    "for attn_layer, module in enumerate(model.transformer.h):\n",
    "    module.register_forward_hook(get_hook(attn_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj['optimal_action'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    final_output = model(batch)\n",
    "    prediction = final_output[:, traj['optimal_action'].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attributions for each layer\n",
    "layer_attributions = {}\n",
    "inputs = format_transformer_input_from_batch(batch, model)\n",
    "baseline_inputs = format_transformer_input_from_batch(baseline_batch, model)\n",
    "\n",
    "for layer_name, layer_output in layer_outputs.items():\n",
    "    # Prepare interpolated inputs\n",
    "    alphas = torch.linspace(0, 1, steps=20)\n",
    "    \n",
    "    all_grads = []\n",
    "    for alpha in alphas:\n",
    "        interp_input = alpha*inputs + (1-alpha)*baseline_inputs\n",
    "        interp_input.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass to get layer output\n",
    "        transformer_output = model.transformer(inputs_embeds=interp_input)\n",
    "        preds = model.pred_actions(transformer_output['last_hidden_state'])\n",
    "        preds = preds[:, -1, :]\n",
    "        target = preds[:, traj['optimal_action'].argmax()]\n",
    "        current_layer_output = layer_outputs[layer_name][0]\n",
    "\n",
    "        grad_wrt_layer = torch.autograd.grad(\n",
    "           outputs=target,\n",
    "           inputs=current_layer_output,\n",
    "           grad_outputs=torch.ones_like(target),\n",
    "           retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        grad_wrt_input = torch.autograd.grad(\n",
    "            outputs=current_layer_output[:, -1],\n",
    "            inputs=interp_input,\n",
    "            grad_outputs=current_layer_output[:, -1],\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        all_grads.append(grad_wrt_input)\n",
    "    \n",
    "    # Average gradients and compute attribution\n",
    "    avg_grad = torch.stack(all_grads).mean(dim=0)\n",
    "    avg_grad = avg_grad.detach().cpu().numpy().squeeze()\n",
    "    delta_input = (inputs - baseline_inputs).detach().cpu().numpy().squeeze()\n",
    "    attribution = np.sum(avg_grad * delta_input, axis=1)\n",
    "    layer_attributions[layer_name] = attribution\n",
    "\n",
    "# Output w.r.t Input\n",
    "alphas = torch.linspace(0, 1, steps=20)\n",
    "all_grads = []\n",
    "for alpha in alphas:\n",
    "    interp_input = alpha*inputs + (1-alpha)*baseline_inputs\n",
    "    interp_input.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass to get layer output\n",
    "    transformer_output = model.transformer(inputs_embeds=interp_input)\n",
    "    preds = model.pred_actions(transformer_output['last_hidden_state'])\n",
    "    preds = preds[:, -1, :]\n",
    "    target = preds[:, traj['optimal_action'].argmax()]\n",
    "    current_layer_output = layer_outputs[layer_name][0]\n",
    "\n",
    "    grad_wrt_input = torch.autograd.grad(\n",
    "        outputs=target,\n",
    "        inputs=interp_input,\n",
    "        grad_outputs=target,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    all_grads.append(grad_wrt_input)\n",
    "avg_grad = torch.stack(all_grads).mean(dim=0)\n",
    "avg_grad = avg_grad.detach().cpu().numpy().squeeze()\n",
    "delta_input = (inputs - baseline_inputs).detach().cpu().numpy().squeeze()\n",
    "input_to_output_attribution = np.sum(avg_grad * delta_input, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import networkx as nx\n",
    "\n",
    "def plot_trajectory(\n",
    "        states, next_states, query_state, attentions, env, figname=None):\n",
    "    \n",
    "    G = env.to_networkx()\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    node_size = 8\n",
    "    goal_node = env.node_map[tuple(env.goal.tolist())]\n",
    "    goal_state = (goal_node.layer, goal_node.pos)\n",
    "    query_node = env.node_map[tuple(query_state.tolist())]\n",
    "    query_state = (query_node.layer, query_node.pos)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(1.5, 1.5))  # Create a new figure\n",
    "    nx.draw(G, pos,\n",
    "            nodelist=[n for n in G.nodes() if n != goal_state],\n",
    "           node_color='gray',\n",
    "           edge_color='white',\n",
    "           node_size=node_size,\n",
    "           font_size=8,\n",
    "           font_weight='bold',\n",
    "           width=1,\n",
    "           alpha=1.)\n",
    "\n",
    "\n",
    "    # Then draw specific nodes in green\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[query_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size)\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                      nodelist=[goal_state],\n",
    "                      node_color='red',\n",
    "                      node_size=node_size,\n",
    "                      node_shape='*')\n",
    "    \n",
    "    for state, next_state, attention in zip(states, next_states, attentions):\n",
    "        edge_color = 'red' if attention > 0 else 'blue'\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "            edgelist=[(tuple(state), tuple(next_state))],\n",
    "            edge_color=edge_color,\n",
    "            alpha=abs(attention),\n",
    "            width=1.5)\n",
    "\n",
    "    # Replace plt.axis('off') with code to add a box outline\n",
    "    plt.axis('on')  # Turn on axes\n",
    "    ax.set_frame_on(True)  # Make sure frame is on\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1) \n",
    "    \n",
    "    # Remove tick marks but keep the box\n",
    "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if figname is not None:\n",
    "        plt.savefig(f'{figname}.png', transparent=True, dpi=300)\n",
    "        plt.savefig(f'{figname}.pdf', transparent=True, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "next_states = []\n",
    "\n",
    "state_features = traj['context_states'][:context_length].squeeze()\n",
    "next_state_features = traj['context_next_states'][:context_length].squeeze()\n",
    "for state_feature, next_state_feature in zip(state_features, next_state_features):\n",
    "    state_feature = tuple(state_feature.tolist())\n",
    "    next_state_feature = tuple(next_state_feature.tolist())\n",
    "    states.append([\n",
    "        env.node_map[state_feature].layer, env.node_map[state_feature].pos])\n",
    "    next_states.append([\n",
    "        env.node_map[next_state_feature].layer, env.node_map[next_state_feature].pos])\n",
    "    \n",
    "states = np.array(states)\n",
    "next_states = np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_and_normalize(attentions, pctile=100):\n",
    "    max_scale = np.percentile(np.abs(attentions), pctile)\n",
    "    attentions = np.clip(attentions, 0, max_scale)/max_scale\n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attentions = input_to_output_attribution[:-1]\n",
    "idx_to_plot = [False if np.array_equal(states[i], next_states[i]) else True for i in range(len(states))]\n",
    "attentions = np.abs(attentions)[idx_to_plot]\n",
    "attentions = clip_and_normalize(attentions)\n",
    "plot_trajectory(\n",
    "    states[idx_to_plot], next_states[idx_to_plot], traj['query_state'],\n",
    "    attentions,\n",
    "    env,\n",
    "    figname=figname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
